"""
LangGraph„Çí‰ΩøÁî®„Åó„Åü„Éó„É≠„É≥„Éó„ÉàÊîπÂñÑ„ÉØ„Éº„ÇØ„Éï„É≠„Éº
"""

import json
from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from dataclasses import dataclass
import anthropic
import logging
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from ..prompt_engine.generator import PromptGenerator, PromptRequest, PromptType
from ..prompt_engine.evaluator import PromptEvaluator, EvaluationResult

logger = logging.getLogger(__name__)


class PromptWorkflowState(TypedDict):
    """„Éó„É≠„É≥„Éó„ÉàÊîπÂñÑ„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÅÆÁä∂ÊÖã"""
    
    # Âü∫Êú¨ÊÉÖÂ†±
    user_request: str
    context: str
    prompt_type: str
    domain: str
    
    # „Éó„É≠„É≥„Éó„ÉàÈñ¢ÈÄ£
    initial_prompt: str
    current_prompt: str
    improved_prompt: str
    
    # Ë©ï‰æ°ÁµêÊûú
    quality_scores: Dict[str, float]
    evaluation_feedback: str
    improvement_suggestions: List[str]
    strengths: List[str]
    weaknesses: List[str]
    
    # „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂà∂Âæ°
    iteration_count: int
    max_iterations: int
    is_satisfactory: bool
    quality_threshold: float
    
    # „É°„ÉÉ„Çª„Éº„Ç∏Â±•Ê≠¥
    messages: Annotated[List[Dict[str, str]], add_messages]
    
    # „É°„Çø„Éá„Éº„Çø
    workflow_start_time: str
    processing_logs: List[str]
    workflow_id: str


@dataclass
class WorkflowConfig:
    """„ÉØ„Éº„ÇØ„Éï„É≠„ÉºË®≠ÂÆö"""
    max_iterations: int = 3
    quality_threshold: float = 8.0
    temperature: float = 0.7
    max_tokens: int = 4000
    model_name: str = "claude-3-sonnet-20240229"
    enable_logging: bool = True


class PromptImprovementWorkflow:
    """„Éó„É≠„É≥„Éó„ÉàÊîπÂñÑ„ÉØ„Éº„ÇØ„Éï„É≠„Éº"""
    
    def __init__(self, anthropic_client: anthropic.AsyncAnthropic, 
                 config: WorkflowConfig):
        self.client = anthropic_client
        self.config = config
        self.generator = PromptGenerator(anthropic_client, config.model_name, config.temperature)
        self.evaluator = PromptEvaluator(anthropic_client, config.model_name)
        self.workflow_history: List[PromptWorkflowState] = []
        
        # StateGraph‰ΩúÊàê
        self.graph = self._create_graph()
    
    def _create_graph(self) -> StateGraph:
        """StateGraph„Çí‰ΩúÊàê"""
        
        # „Éé„Éº„ÉâÈñ¢Êï∞„ÅÆÂÆöÁæ©
        async def generate_node(state: PromptWorkflowState) -> PromptWorkflowState:
            """„Éó„É≠„É≥„Éó„ÉàÁîüÊàê„Éé„Éº„Éâ"""
            try:
                if state["iteration_count"] == 0:
                    # ÂàùÂõû„ÅØÂàùÊúü„Éó„É≠„É≥„Éó„ÉàÁîüÊàê
                    request = PromptRequest(
                        prompt_type=PromptType(state["prompt_type"]),
                        user_requirements=state["user_request"],
                        context=state["context"],
                        domain=state["domain"]
                    )
                    
                    result = await self.generator.generate_prompt(request)
                    
                    state["initial_prompt"] = result.content
                    state["current_prompt"] = result.content
                    state["processing_logs"].append(
                        f"‚úÖ ÂàùÊúü„Éó„É≠„É≥„Éó„ÉàÁîüÊàêÂÆå‰∫Ü: {len(result.content)}ÊñáÂ≠ó"
                    )
                else:
                    # 2ÂõûÁõÆ‰ª•Èôç„ÅØÊîπÂñÑ„Éó„É≠„É≥„Éó„ÉàÁîüÊàê
                    # ÁèæÂú®„ÅÆ„Éó„É≠„É≥„Éó„Éà„Åã„Çâ GeneratedPrompt „Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩúÊàê
                    from ..prompt_engine.generator import GeneratedPrompt
                    current_generated_prompt = GeneratedPrompt(
                        content=state["current_prompt"],
                        metadata={"prompt_type": state["prompt_type"]}
                    )
                    
                    result = await self.generator.improve_prompt(
                        current_generated_prompt,
                        state["evaluation_feedback"],
                        state["improvement_suggestions"]
                    )
                    
                    state["improved_prompt"] = result.content
                    state["current_prompt"] = result.content
                    state["iteration_count"] += 1
                    state["processing_logs"].append(
                        f"üîÑ „Éó„É≠„É≥„Éó„ÉàÊîπÂñÑÂÆå‰∫Ü (Á¨¨{state['iteration_count']}Âõû): {len(result.content)}ÊñáÂ≠ó"
                    )
                
                # „É°„ÉÉ„Çª„Éº„Ç∏Â±•Ê≠¥Êõ¥Êñ∞
                state["messages"].append({
                    "role": "assistant",
                    "content": f"„Éó„É≠„É≥„Éó„ÉàÁîüÊàê/ÊîπÂñÑÂÆå‰∫Ü (ÂèçÂæ©{state['iteration_count']}ÂõûÁõÆ)"
                })
                
                return state
                
            except Exception as e:
                state["processing_logs"].append(f"‚ùå „Éó„É≠„É≥„Éó„ÉàÁîüÊàê„Ç®„É©„Éº: {str(e)}")
                raise
        
        async def evaluate_node(state: PromptWorkflowState) -> PromptWorkflowState:
            """ÂìÅË≥™Ë©ï‰æ°„Éé„Éº„Éâ"""
            try:
                result = await self.evaluator.evaluate_prompt(
                    prompt_content=state["current_prompt"],
                    original_request=state["user_request"],
                    context=state["context"]
                )
                
                # Ë©ï‰æ°ÁµêÊûú„ÇíÁä∂ÊÖã„Å´ÂèçÊò†
                state["quality_scores"] = result.metrics.to_dict()
                state["evaluation_feedback"] = result.feedback
                state["improvement_suggestions"] = result.suggestions
                state["strengths"] = result.strengths
                state["weaknesses"] = result.weaknesses
                
                # ÂìÅË≥™Âü∫Ê∫ñÈÅîÊàê„ÉÅ„Çß„ÉÉ„ÇØ
                overall_score = result.metrics.overall_score()
                state["is_satisfactory"] = overall_score >= state["quality_threshold"]
                
                state["processing_logs"].append(
                    f"üìä ÂìÅË≥™Ë©ï‰æ°ÂÆå‰∫Ü: Á∑èÂêà„Çπ„Ç≥„Ç¢ {overall_score:.1f}/10 "
                    f"({'‚úÖ Âü∫Ê∫ñÈÅîÊàê' if state['is_satisfactory'] else '‚ö†Ô∏è ÊîπÂñÑ„ÅåÂøÖË¶Å'})"
                )
                
                # „É°„ÉÉ„Çª„Éº„Ç∏Â±•Ê≠¥Êõ¥Êñ∞
                state["messages"].append({
                    "role": "assistant",
                    "content": f"ÂìÅË≥™Ë©ï‰æ°ÂÆå‰∫Ü: {overall_score:.1f}/10"
                })
                
                return state
                
            except Exception as e:
                state["processing_logs"].append(f"‚ùå ÂìÅË≥™Ë©ï‰æ°„Ç®„É©„Éº: {str(e)}")
                raise
        
        def finalize_node(state: PromptWorkflowState) -> PromptWorkflowState:
            """ÊúÄÁµÇÂåñ„Éé„Éº„Éâ"""
            final_score = state["quality_scores"].get("overall", 0)
            state["processing_logs"].append(
                f"‚ú® „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆå‰∫Ü: {state['iteration_count']}Âõû„ÅÆÊîπÂñÑ, "
                f"ÊúÄÁµÇ„Çπ„Ç≥„Ç¢ {final_score:.1f}/10"
            )
            
            # „É°„ÉÉ„Çª„Éº„Ç∏Â±•Ê≠¥Êõ¥Êñ∞
            state["messages"].append({
                "role": "assistant",
                "content": f"„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆå‰∫Ü: ÊúÄÁµÇ„Çπ„Ç≥„Ç¢ {final_score:.1f}/10"
            })
            
            return state
        
        def should_continue(state: PromptWorkflowState) -> str:
            """Á∂ôÁ∂öÂà§ÂÆöÈñ¢Êï∞"""
            # ÂìÅË≥™Âü∫Ê∫ñ„ÇíÊ∫Ä„Åü„Åó„Å¶„ÅÑ„Çå„Å∞ÁµÇ‰∫Ü
            if state["is_satisfactory"]:
                return "finalize"
            
            # ÊúÄÂ§ßÂèçÂæ©ÂõûÊï∞„Å´ÈÅî„Åó„Å¶„ÅÑ„Çå„Å∞ÁµÇ‰∫Ü
            if state["iteration_count"] >= state["max_iterations"]:
                return "finalize"
            
            return "generate"
        
        # „Ç∞„É©„ÉïÊßãÁØâ
        graph = StateGraph(PromptWorkflowState)
        
        # „Éé„Éº„ÉâËøΩÂä†
        graph.add_node("generate", generate_node)
        graph.add_node("evaluate", evaluate_node)
        graph.add_node("finalize", finalize_node)
        
        # „Ç®„ÉÉ„Ç∏Ë®≠ÂÆö
        graph.set_entry_point("generate")
        graph.add_edge("generate", "evaluate")
        graph.add_conditional_edges(
            "evaluate",
            should_continue,
            {
                "generate": "generate",
                "finalize": "finalize"
            }
        )
        graph.set_finish_point("finalize")
        
        return graph
    
    def create_initial_state(self, user_request: str, context: str = "",
                           prompt_type: str = "general_poc", domain: str = "") -> PromptWorkflowState:
        """ÂàùÊúüÁä∂ÊÖã„Çí‰ΩúÊàê"""
        workflow_id = f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return {
            "user_request": user_request,
            "context": context,
            "prompt_type": prompt_type,
            "domain": domain,
            "initial_prompt": "",
            "current_prompt": "",
            "improved_prompt": "",
            "quality_scores": {},
            "evaluation_feedback": "",
            "improvement_suggestions": [],
            "strengths": [],
            "weaknesses": [],
            "iteration_count": 0,
            "max_iterations": self.config.max_iterations,
            "is_satisfactory": False,
            "quality_threshold": self.config.quality_threshold,
            "messages": [],
            "workflow_start_time": datetime.now().isoformat(),
            "processing_logs": [],
            "workflow_id": workflow_id
        }
    
    async def run_workflow(self, user_request: str, context: str = "",
                          prompt_type: str = "general_poc", domain: str = "") -> PromptWorkflowState:
        """„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÇíÂÆüË°å"""
        # ÂàùÊúüÁä∂ÊÖã‰ΩúÊàê
        initial_state = self.create_initial_state(user_request, context, prompt_type, domain)
        initial_state["processing_logs"].append(f"üöÄ „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÈñãÂßã: {initial_state['workflow_id']}")
        
        try:
            # „É°„É¢„É™„Çª„Éº„Éê„ÉºË®≠ÂÆöÔºà„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÊ©üËÉΩÔºâ
            memory = MemorySaver()
            graph_with_memory = self.graph.compile(checkpointer=memory)
            
            # „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆüË°å
            config = {"configurable": {"thread_id": initial_state["workflow_id"]}}
            
            final_state = None
            async for state in graph_with_memory.astream(initial_state, config):
                final_state = state
                if self.config.enable_logging:
                    logger.info(f"„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁä∂ÊÖãÊõ¥Êñ∞: {list(state.keys())}")
            
            if final_state is None:
                raise RuntimeError("„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÅÆÂÆüË°åÁµêÊûú„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü")
            
            # ÊúÄÁµÇÁä∂ÊÖã„Åã„ÇâÂÆüÈöõ„ÅÆÂÄ§„ÇíÂèñÂæóÔºàLangGraph„ÅÆ„É©„ÉÉ„Éó„ÇíËß£Èô§Ôºâ
            final_workflow_state = list(final_state.values())[0]
            
            # Â±•Ê≠¥„Å´ËøΩÂä†
            self.workflow_history.append(final_workflow_state)
            
            logger.info(f"„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆå‰∫Ü: {final_workflow_state['workflow_id']}")
            return final_workflow_state
            
        except Exception as e:
            logger.error(f"„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆüË°å„Ç®„É©„Éº: {str(e)}")
            initial_state["processing_logs"].append(f"üí• „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÂÆüË°å„Ç®„É©„Éº: {str(e)}")
            raise
    
    def get_workflow_statistics(self) -> Dict[str, Any]:
        """„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁµ±Ë®à„ÇíÂèñÂæó"""
        if not self.workflow_history:
            return {"total_workflows": 0}
        
        stats = {
            "total_workflows": len(self.workflow_history),
            "average_iterations": 0,
            "success_rate": 0,
            "average_final_score": 0,
            "by_prompt_type": {},
            "recent_workflows": []
        }
        
        total_iterations = 0
        successful_workflows = 0
        total_final_score = 0
        type_counts = {}
        
        for workflow in self.workflow_history:
            total_iterations += workflow["iteration_count"]
            
            if workflow["is_satisfactory"]:
                successful_workflows += 1
            
            final_score = workflow["quality_scores"].get("overall", 0)
            total_final_score += final_score
            
            prompt_type = workflow["prompt_type"]
            if prompt_type not in type_counts:
                type_counts[prompt_type] = {"count": 0, "avg_score": 0, "total_score": 0}
            
            type_counts[prompt_type]["count"] += 1
            type_counts[prompt_type]["total_score"] += final_score
        
        stats["average_iterations"] = total_iterations / len(self.workflow_history)
        stats["success_rate"] = successful_workflows / len(self.workflow_history)
        stats["average_final_score"] = total_final_score / len(self.workflow_history)
        
        # „Çø„Ç§„ÉóÂà•Áµ±Ë®à
        for prompt_type, data in type_counts.items():
            stats["by_prompt_type"][prompt_type] = {
                "count": data["count"],
                "average_score": data["total_score"] / data["count"]
            }
        
        # ÊúÄËøë„ÅÆ„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÔºàÊúÄÊñ∞5‰ª∂Ôºâ
        stats["recent_workflows"] = [
            {
                "workflow_id": workflow["workflow_id"],
                "prompt_type": workflow["prompt_type"],
                "iteration_count": workflow["iteration_count"],
                "final_score": workflow["quality_scores"].get("overall", 0),
                "is_satisfactory": workflow["is_satisfactory"],
                "start_time": workflow["workflow_start_time"]
            }
            for workflow in self.workflow_history[-5:]
        ]
        
        return stats
    
    def export_workflow_results(self, filepath: str):
        """„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁµêÊûú„Çí„Ç®„ÇØ„Çπ„Éù„Éº„Éà"""
        try:
            export_data = {
                "export_timestamp": datetime.now().isoformat(),
                "total_workflows": len(self.workflow_history),
                "config": {
                    "max_iterations": self.config.max_iterations,
                    "quality_threshold": self.config.quality_threshold,
                    "model_name": self.config.model_name
                },
                "statistics": self.get_workflow_statistics(),
                "workflows": self.workflow_history
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁµêÊûú„Çí„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åó„Åæ„Åó„Åü: {filepath}")
            
        except Exception as e:
            logger.error(f"„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Ç®„É©„Éº: {str(e)}")
            raise